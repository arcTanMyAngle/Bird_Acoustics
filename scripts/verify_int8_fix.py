#!/usr/bin/env python3
"""verify_int8_fix.py - Corrected verification for int8 quantized TFLite models"""

import numpy as np
import torch
from pathlib import Path


def verify_int8_model_with_litert(
    pytorch_model: torch.nn.Module,
    tflite_path: Path,
    n_tests: int = 20
):
    """Verify int8 TFLite model using ai_edge_litert with proper input quantization."""
    from ai_edge_litert.interpreter import Interpreter  # Fixed import
    
    print(f"\nüî¨ Verifying Int8 Model ({n_tests} test samples)...")
    print(f"   Using: {tflite_path}")
    
    # Load TFLite model
    interpreter = Interpreter(model_path=str(tflite_path))
    interpreter.allocate_tensors()
    
    # Get input/output details
    input_details = interpreter.get_input_details()[0]
    output_details = interpreter.get_output_details()[0]
    
    print(f"\n   Input details:")
    print(f"      Shape: {input_details['shape']}")
    print(f"      Dtype: {input_details['dtype']}")
    
    input_is_quantized = input_details['dtype'] == np.int8
    if input_is_quantized:
        input_scale = input_details['quantization_parameters']['scales'][0]
        input_zp = input_details['quantization_parameters']['zero_points'][0]
        print(f"      Quantization: scale={input_scale:.6f}, zp={input_zp}")
    
    print(f"\n   Output details:")
    print(f"      Dtype: {output_details['dtype']}")
    
    output_is_quantized = output_details['dtype'] == np.int8
    if output_is_quantized:
        output_scale = output_details['quantization_parameters']['scales'][0]
        output_zp = output_details['quantization_parameters']['zero_points'][0]
        print(f"      Quantization: scale={output_scale:.6f}, zp={output_zp}")
    
    similarities = []
    
    for i in range(n_tests):
        test_input_float = torch.randn(1, 1, 40, 188)
        
        # PyTorch inference
        with torch.no_grad():
            pytorch_output = pytorch_model(test_input_float).numpy().flatten()
        
        # Quantize input if needed
        if input_is_quantized:
            test_input_np = test_input_float.numpy()
            test_input_quant = np.round(test_input_np / input_scale + input_zp)
            test_input_quant = np.clip(test_input_quant, -128, 127).astype(np.int8)
        else:
            test_input_quant = test_input_float.numpy().astype(np.float32)
        
        # TFLite inference
        interpreter.set_tensor(input_details['index'], test_input_quant)
        interpreter.invoke()
        tflite_output_raw = interpreter.get_tensor(output_details['index'])
        
        # Dequantize output if needed
        if output_is_quantized:
            tflite_output = (tflite_output_raw.astype(np.float32) - output_zp) * output_scale
        else:
            tflite_output = tflite_output_raw.astype(np.float32)
        
        tflite_output = tflite_output.flatten()
        
        # Cosine similarity
        cos_sim = np.dot(pytorch_output, tflite_output) / (
            np.linalg.norm(pytorch_output) * np.linalg.norm(tflite_output) + 1e-8
        )
        similarities.append(cos_sim)
        
        if (i + 1) % 5 == 0:
            print(f"   Sample {i + 1}: cos_sim={cos_sim:.6f}")
    
    avg_sim = float(np.mean(similarities))
    min_sim = float(np.min(similarities))
    
    print(f"\n   üìä Results:")
    print(f"      Average: {avg_sim:.6f}")
    print(f"      Minimum: {min_sim:.6f}")
    
    return avg_sim, min_sim


def main():
    import sys
    sys.path.insert(0, str(Path(__file__).parent))
    from export_tflite import load_model
    
    project_dir = Path(__file__).parent.parent
    model_path = project_dir / "models" / "lottery_ticket" / "winning_ticket_79pct.pth"
    tflite_path = project_dir / "models" / "exported" / "bird_classifier_int8.tflite"
    
    if not tflite_path.exists():
        print(f"‚ùå TFLite model not found: {tflite_path}")
        return 1
    
    print("=" * 60)
    print("üî¨ Int8 Model Verification (Corrected)")
    print("=" * 60)
    
    model, classes = load_model(model_path)
    avg_sim, min_sim = verify_int8_model_with_litert(model, tflite_path)
    
    print("\n" + "=" * 60)
    print("üéÆ BOSS FIGHT: Int8 Verification")
    print("=" * 60)
    
    # Int8 typically has 0.85-0.95 similarity due to quantization
    if min_sim >= 0.85:
        print(f"üèÜ PASSED! Min similarity {min_sim:.4f} >= 0.85")
        return 0
    else:
        print(f"‚ö†Ô∏è  Min similarity {min_sim:.4f} - may still work on device")
        return 0


if __name__ == "__main__":
    import sys
    sys.exit(main())